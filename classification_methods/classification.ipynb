{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from warnings import filterwarnings\n",
    "from sklearn.discriminant_analysis import (LinearDiscriminantAnalysis,\n",
    "                                           QuadraticDiscriminantAnalysis)\n",
    "\n",
    "__author__ = 'Benyamin Jafari'\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common section (methods definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(apply_pca=False):\n",
    "    df = pd.read_csv('dataset/blocks.csv')\n",
    "    y = df.iloc[:, 0]  # select first column.\n",
    "    y = LabelEncoder().fit_transform(y)  # Encoding labels to numbers.\n",
    "    X = df.iloc[:, 1:]  # remove first column.\n",
    "\n",
    "    if not apply_pca:\n",
    "        return train_test_split(X, y, test_size=.1)\n",
    "    else:\n",
    "        pca_ = PCA(whiten=True)\n",
    "        pca_.fit(X)\n",
    "        X_pca = pca_.transform(X)\n",
    "        return train_test_split(X_pca, y, test_size=.1)\n",
    "\n",
    "def knn_predictor(k=None):\n",
    "    \"\"\"\n",
    "    KNN method.\n",
    "    :param k: k-neighbors value.\n",
    "    :return: If `k` be None, it returns best `k` in training mode, else\n",
    "    it returns the predict result for the test set by the chosen `k`.\n",
    "    \"\"\"\n",
    "    if k is None:\n",
    "        k_values = np.arange(1, 7)\n",
    "\n",
    "        '''Cross validation to find the best number of neighbors(k).'''\n",
    "        folds = 10  # number of folds.\n",
    "        cv_scores = []\n",
    "        for i, k in enumerate(k_values):\n",
    "            knn_ = KNeighborsClassifier(n_neighbors=k, metric='minkowski', p=2)\n",
    "            cv_scores.append(\n",
    "                np.mean(cross_val_score(knn_, X_train, y_train, cv=folds)))\n",
    "\n",
    "        best_k = cv_scores.index(max(cv_scores)) + 1\n",
    "    else:\n",
    "        best_k = k\n",
    "\n",
    "    knn_ = KNeighborsClassifier(n_neighbors=best_k, metric='minkowski', p=2)\n",
    "    knn_.fit(X_train, y_train)\n",
    "    accuracy = knn_.score(X_test, y_test)\n",
    "\n",
    "    return best_k, accuracy\n",
    "\n",
    "def parametric_classifications():\n",
    "    logreg = LogisticRegression(\n",
    "        multi_class=\"multinomial\",\n",
    "        solver=\"newton-cg\",  # I also tried by 'saga'\n",
    "        # penalty='none' # I also use this penalty.\n",
    "    )\n",
    "    logreg.fit(X_train, y_train)\n",
    "\n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "    lda.fit(X_train, y_train)\n",
    "\n",
    "    qda = QuadraticDiscriminantAnalysis()\n",
    "    qda.fit(X_train, y_train)\n",
    "\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(X_train, y_train)\n",
    "\n",
    "    logreg_acc = logreg.score(X_test, y_test)\n",
    "    lda_acc = lda.score(X_test, y_test)\n",
    "    qda_acc = qda.score(X_test, y_test)\n",
    "    gnb_acc = gnb.score(X_test, y_test)\n",
    "\n",
    "    return logreg_acc, lda_acc, qda_acc, gnb_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################Part a:################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing sets ...\n",
      "All done.\n"
     ]
    }
   ],
   "source": [
    "print('Initializing sets ...')\n",
    "X_train, X_test, y_train, y_test = init()\n",
    "print(\"All done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################Part b:################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best K is: 4\n"
     ]
    }
   ],
   "source": [
    "best_k, _ = knn_predictor()\n",
    "print(f'The best K is: {best_k}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################Part c:################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It takes few minutes, please wait ...\n",
      "logistic regression    0.923382\n",
      "LDA                    0.764559\n",
      "QDA                    0.826074\n",
      "NBC                    0.715235\n",
      "KNN                    0.879985\n",
      "PCA-based                   NaN\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "average_table = pd.DataFrame(\n",
    "    columns=['logistic regression', 'LDA', 'QDA', 'NBC', 'KNN', 'PCA-based'],\n",
    "    index=range(1000))\n",
    "\n",
    "print('It takes few minutes, please wait ...')\n",
    "for i, _ in average_table.iterrows():\n",
    "    X_train, X_test, y_train, y_test = init()\n",
    "    # best_k, _ = knn_predictor()\n",
    "    _, average_table.iloc[i, 4] = knn_predictor(k=best_k)\n",
    "    logreg_acc, lda_acc, qda_acc, gnb_acc = parametric_classifications()\n",
    "    average_table.iloc[i, 0] = logreg_acc\n",
    "    average_table.iloc[i, 1] = lda_acc\n",
    "    average_table.iloc[i, 2] = qda_acc\n",
    "    average_table.iloc[i, 3] = gnb_acc\n",
    "\n",
    "print(average_table.mean(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################Part d:################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Confusion Matrix:\n",
      " [[ 2  0  0  0  0]\n",
      " [ 0 24  0  2  3]\n",
      " [ 0  0 11  2  0]\n",
      " [ 0  3  1 11  0]\n",
      " [ 0  1  1  0  7]]\n",
      "QDA Confusion Matrix:\n",
      " [[ 2  0  0  0  0]\n",
      " [ 0 21  0  2  6]\n",
      " [ 0  0  8  3  2]\n",
      " [ 0  0  0 14  1]\n",
      " [ 1  0  0  0  8]]\n",
      "NBC Confusion Matrix:\n",
      " [[ 2  0  0  0  0]\n",
      " [ 0 24  0  2  3]\n",
      " [ 4  0  3  2  4]\n",
      " [ 0  0  0 12  3]\n",
      " [ 0  0  0  0  9]]\n",
      "LogReg Confusion Matrix:\n",
      " [[ 1  0  0  0  1]\n",
      " [ 0 28  0  1  0]\n",
      " [ 0  0 13  0  0]\n",
      " [ 0  1  1 13  0]\n",
      " [ 1  0  0  0  8]]\n",
      "KNN Confusion Matrix:\n",
      " [[ 0  1  1  0  0]\n",
      " [ 0 28  0  1  0]\n",
      " [ 0  0 13  0  0]\n",
      " [ 1  1  1 12  0]\n",
      " [ 0  0  1  0  8]]\n",
      "LDA Accuracy:  0.8088235294117647\n",
      "QDA Accuracy:  0.7794117647058824\n",
      "NBC Accuracy:  0.7352941176470589\n",
      "LogReg Accuracy:  0.9264705882352942\n",
      "KNN Accuracy:  0.8970588235294118\n",
      "The best predictor in this case is Logistic Regression\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = init()\n",
    "\n",
    "knn_ = KNeighborsClassifier(n_neighbors=best_k, metric='minkowski', p=2)\n",
    "knn_.fit(X_train, y_train)\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train, y_train)\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "qda.fit(X_train, y_train)\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "logreg = LogisticRegression(\n",
    "    multi_class=\"multinomial\",\n",
    "    solver=\"newton-cg\",  # I also tried by 'saga'\n",
    "    # penalty='none' # I also use this penalty.\n",
    ")\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "print(\n",
    "    'LDA Confusion Matrix:\\n',\n",
    "    confusion_matrix(y_test, lda.predict(X_test)))\n",
    "print(\n",
    "    'QDA Confusion Matrix:\\n',\n",
    "    confusion_matrix(y_test, qda.predict(X_test)))\n",
    "print(\n",
    "    'NBC Confusion Matrix:\\n',\n",
    "    confusion_matrix(y_test, gnb.predict(X_test)))\n",
    "print(\n",
    "    'LogReg Confusion Matrix:\\n',\n",
    "    confusion_matrix(y_test, logreg.predict(X_test)))\n",
    "print(\n",
    "    'KNN Confusion Matrix:\\n',\n",
    "    confusion_matrix(y_test, knn_.predict(X_test)))\n",
    "\n",
    "logreg_acc = logreg.score(X_test, y_test)\n",
    "lda_acc = lda.score(X_test, y_test)\n",
    "qda_acc = qda.score(X_test, y_test)\n",
    "gnb_acc = gnb.score(X_test, y_test)\n",
    "knn_acc = knn_.score(X_test, y_test)\n",
    "print('LDA Accuracy: ', lda_acc)\n",
    "print('QDA Accuracy: ', qda_acc)\n",
    "print('NBC Accuracy: ', gnb_acc)\n",
    "print('LogReg Accuracy: ', logreg_acc)\n",
    "print('KNN Accuracy: ', knn_acc)\n",
    "print('The best predictor in this case is Logistic Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix is a square matrix, which in the ideal case, its main diagonal must be valued and other sides must be none.\n",
    "\n",
    "```\n",
    "    Confusion Matrix in binomial class:\n",
    "    ------+-------\n",
    "    | TP  |  FP  |\n",
    "    --------------\n",
    "    | FN  |  TN  |\n",
    "    ------+-------\n",
    "```\n",
    "\n",
    "Thus, there are two main methods to evaluate a predictor: Using `accuracy` or `F1-score` (in imbalanced cases should be used).\n",
    "\n",
    "`ACC = (TP + TN) / (P + N)`\n",
    "\n",
    "`F1 = 2TP / (2TP + FP + FN)`\n",
    "\n",
    "[Here is my post](https://stats.stackexchange.com/a/441689/209206)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################Part e:################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best number of PC scores is 10\n",
      "It takes few minutes, please wait ...\n",
      "logistic regression    0.923382\n",
      "LDA                    0.764559\n",
      "QDA                    0.826074\n",
      "NBC                    0.715235\n",
      "KNN                    0.879985\n",
      "PCA-based              0.883926\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "X_train_pca, X_test_pca, y_train, y_test = init(apply_pca=True)\n",
    "logreg = LogisticRegression(\n",
    "    multi_class=\"multinomial\",\n",
    "    solver=\"newton-cg\")\n",
    "\n",
    "cv_scores = []\n",
    "for dim in range(1, X_train.shape[1] + 1):\n",
    "    cv_scores.append(\n",
    "        np.mean(cross_val_score(\n",
    "            logreg, X_train_pca[:, :dim], y_train, cv=10)))\n",
    "\n",
    "best_pca = (cv_scores.index(max(cv_scores)) + 1)\n",
    "print(f\"The best number of PC scores is {best_pca}\")\n",
    "\n",
    "# plt.bar(np.arange(1, len(pca.explained_variance_ratio_)+1), pca.explained_variance_,\n",
    "#         color=\"blue\",\n",
    "#         edgecolor=\"Red\")\n",
    "# plt.show()\n",
    "#\n",
    "# logreg = LogisticRegression(\n",
    "#     multi_class=\"multinomial\",\n",
    "#     solver=\"newton-cg\")\n",
    "# logreg.fit(X_train_pca[:, :2], y_train)\n",
    "# logreg.score(X_test_pca[:, :2], y_test)\n",
    "\n",
    "print('It takes few minutes, please wait ...')\n",
    "logreg = LogisticRegression(\n",
    "    multi_class=\"multinomial\",\n",
    "    solver=\"newton-cg\")\n",
    "for i, _ in average_table.iterrows():\n",
    "    X_train_pca, X_test_pca, y_train, y_test = init(apply_pca=True)\n",
    "    logreg.fit(X_train_pca[:, :best_pca], y_train)\n",
    "    average_table.iloc[i, -1] = logreg.score(X_test_pca[:, :best_pca], y_test)\n",
    "\n",
    "print(average_table.mean(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above cell, in commented lines, I reached to two PC scores by its histogram. Its accuracy was about 65% which is very low rather than 10 PC scores which obtained from cross-validation. However, it's obvious when 10 PC scores chosen as the best PC score (note: all columns is 10) its predictor accuracy should be lower than without applying PCA. As you can see, the PCA-based accuracy is 88% and without it is 92%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
